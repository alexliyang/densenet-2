{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIP: Trying to implement the above in tf-slim\n",
    "##### Implementation in Slim on MNIST\n",
    "##### ... so far not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "Y_train = np_utils.to_categorical(Y_train, n_classes)\n",
    "Y_test = np_utils.to_categorical(Y_test, n_classes)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the DenseNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bn_relu_conv(net, n_channels, drop, is_training=True):\n",
    "     with slim.arg_scope([slim.conv2d], padding = 'SAME',\n",
    "                        activation_fn=tf.nn.relu):\n",
    "        net = tf.contrib.layers.batch_norm(net)\n",
    "        net = slim.conv2d(net, n_channels, [3, 3])\n",
    "        net = slim.dropout(net, drop, is_training=is_training)\n",
    "        return net\n",
    "\n",
    "def add_layer(net, n_channels, drop, is_training = True):\n",
    "    net = bn_relu_conv(net, n_channels, drop, is_training)\n",
    "    return net\n",
    "\n",
    "def transition(net, n_channels, drop):\n",
    "    net = bn_relu_conv(net, n_channels, drop)\n",
    "    net = slim.max_pool2d(net, [2, 2], stride=[2, 2], padding='SAME')\n",
    "    return net\n",
    "\n",
    "def bn_relu_conv_model(input, start_channels, growth_rate, n_classes, levels, drop, is_training):\n",
    "        \n",
    "    with slim.arg_scope([slim.conv2d], padding = 'SAME', activation_fn=tf.nn.relu):\n",
    "        with slim.arg_scope([slim.fully_connected], activation_fn=tf.nn.relu):\n",
    "            n_channels = start_channels\n",
    "            net = slim.conv2d(input, n_channels, [3, 3])\n",
    "\n",
    "            for i in range(levels):\n",
    "                add_layer(net, n_channels, drop, is_training)\n",
    "                n_channels += growth_rate\n",
    "            transition(net, n_channels, drop)\n",
    "\n",
    "            net = tf.contrib.layers.batch_norm(net)\n",
    "            net = slim.conv2d(net, n_channels, [3, 3])\n",
    "            net = slim.flatten(net)\n",
    "            net = slim.fully_connected(net, n_classes)\n",
    "            return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ckpt_dir = '/tmp/regression_model/'\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    start_channels = 16\n",
    "    n_classes = 10\n",
    "    drop = 0.2\n",
    "    growth_rate = 12\n",
    "    levels = 10\n",
    "    \n",
    "    ####have code for one hot labels, but the input data is already coded so this is unncessary in this instance\n",
    "#one_hot_labels = slim.one_hot_encoding(Y_train, n_classes)\n",
    "\n",
    "logits = bn_relu_conv_model(X_train, start_channels, growth_rate, n_classes, levels, drop, True)\n",
    "\n",
    "loss = slim.losses.softmax_cross_entropy(logits, Y_train)\n",
    "\n",
    "optimizer = tf.train.AdadeltaOptimizer(2e-5)\n",
    "\n",
    "    train_op = slim.learning.create_train_op(loss, optimizer) \n",
    "\n",
    "    # Run the training inside a session.\n",
    "    final_loss = slim.learning.train(\n",
    "        train_op,\n",
    "        logdir=ckpt_dir,\n",
    "        number_of_steps=5000,\n",
    "        save_summaries_secs=5,\n",
    "        log_every_n_steps=500)\n",
    "    \n",
    "print(\"Finished training. Last batch loss:\", final_loss)\n",
    "print(\"Checkpoint saved in %s\" % ckpt_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "  \n",
    "    # Create the model structure. (Parameters will be loaded below.)\n",
    "    Y_out = bn_relu_conv_model(X_train, n_channels, growth_rate, \n",
    "                            n_classes, levels, drop, False)\n",
    "\n",
    "    # Make a session which restores the old parameters from a checkpoint.\n",
    "    sv = tf.train.Supervisor(logdir=ckpt_dir)\n",
    "    with sv.managed_session() as sess:\n",
    "        X_test, Y_out, Y_test = sess.run([X_test, Y_out, Y_test])\n",
    "\n",
    "plt.scatter(X_test, Y_test, c='r');\n",
    "plt.scatter(X_test, Y_out, c='b');\n",
    "plt.title('red=true, blue=predicted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
