{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of DenseNet in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "#from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get MNIST data to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "Y_train = np_utils.to_categorical(Y_train, n_classes)\n",
    "Y_test = np_utils.to_categorical(Y_test, n_classes)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train.shape\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet \n",
    "##### With default parameters for Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#single layer with batchnorm, relu, conv2D and a dropout\n",
    "def bn_relu_conv(model, num_filter, drop): \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(num_filter, 3, 3))\n",
    "    model.add(Dropout(drop))\n",
    "    return model\n",
    "\n",
    "# function for adding layers\n",
    "def add_layer(model, num_filter, drop):\n",
    "    bn_relu_conv(model, num_filter , drop)\n",
    "    return model\n",
    "\n",
    "#function for generating transitions between blocks\n",
    "def transition(model, num_filter, drop):\n",
    "    bn_relu_conv(model, num_filter, drop)\n",
    "    MaxPooling2D(pool_size=(2, 2))\n",
    "    return model\n",
    "\n",
    "n_classes = 10\n",
    "drop = 0.2         \n",
    "start_channels = 16\n",
    "growth_rate = 12\n",
    "\n",
    "#initial layer is a basic Conv2D\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(start_channels, 3, 3, border_mode = 'valid', input_shape = (1, 28, 28)))\n",
    "\n",
    "n_channels = start_channels\n",
    "\n",
    "#make a dense block (10 layers deep) with parameters growing by a small value (12) with each layer (+ previous layers)\n",
    "for i in range(10):\n",
    "    add_layer(model, n_channels, drop)\n",
    "    n_channels += growth_rate\n",
    "transition(model, n_channels, drop)\n",
    "\n",
    "#currently using a single dense block (paper did 3 dense blocks with transitions and total layers of 40 or 100) \n",
    "#final classification layer \n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(n_classes))\n",
    "model.add(Activation('softmax'))\n",
    "    \n",
    "#Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, Y_train, nb_epoch=10, batch_size=64, validation_data=(X_test, Y_test),\n",
    "              shuffle=True)\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Output for the code above (with Adam optimizer)\n",
    "Train on 60000 samples, validate on 10000 samples\n",
    "Epoch 1/10\n",
    "60000/60000 [==============================] - 1218s - loss: 0.6435 - acc: 0.7790 - val_loss: 0.7873 - val_acc: 0.7249\n",
    "Epoch 2/10\n",
    "60000/60000 [==============================] - 1231s - loss: 0.2497 - acc: 0.9213 - val_loss: 0.4668 - val_acc: 0.8565\n",
    "Epoch 3/10\n",
    "60000/60000 [==============================] - 1226s - loss: 0.1875 - acc: 0.9418 - val_loss: 0.3019 - val_acc: 0.9112\n",
    "Epoch 4/10\n",
    "60000/60000 [==============================] - 1249s - loss: 0.1585 - acc: 0.9497 - val_loss: 0.2843 - val_acc: 0.9147\n",
    "Epoch 5/10\n",
    "60000/60000 [==============================] - 1278s - loss: 0.1377 - acc: 0.9566 - val_loss: 0.3219 - val_acc: 0.9000\n",
    "Epoch 6/10\n",
    " 4560/60000 [=>............................] - ETA: 1098s - loss: 0.1140 - acc: 0.9638"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same DenseNet as above except with Adadelta optimizer and no real tuning of parameters (appears to be a bit better than default Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bn_relu_conv(model, num_filter, drop): \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(num_filter, 3, 3))\n",
    "    model.add(Dropout(drop))\n",
    "    return model\n",
    "\n",
    "def add_layer(model, num_filter, drop):\n",
    "    bn_relu_conv(model, num_filter , drop)\n",
    "    return model\n",
    "\n",
    "def transition(model, num_filter, drop):\n",
    "    bn_relu_conv(model, num_filter, drop)\n",
    "    MaxPooling2D(pool_size=(2, 2))\n",
    "    return model\n",
    "\n",
    "n_classes = 10\n",
    "drop = 0.2         \n",
    "start_channels = 16\n",
    "growth_rate = 12\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(start_channels, 3, 3, border_mode = 'valid', input_shape = (1, 28, 28)))\n",
    "\n",
    "n_channels = start_channels\n",
    "\n",
    "for i in range(10):\n",
    "    add_layer(model, n_channels, drop)\n",
    "    n_channels += growth_rate\n",
    "transition(model, n_channels, drop)\n",
    "\n",
    "    \n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(n_classes))\n",
    "model.add(Activation('softmax'))\n",
    "    \n",
    "#adam_opt = Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer = 'adadelta',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, Y_train, nb_epoch=10, batch_size=64, validation_data=(X_test, Y_test),\n",
    "              shuffle=True)\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train on 60000 samples, validate on 10000 samples\n",
    "Epoch 1/10\n",
    "60000/60000 [==============================] - 1422s - loss: 0.4466 - acc: 0.8525 - val_loss: 0.0918 - val_acc: 0.9690\n",
    "Epoch 2/10\n",
    "60000/60000 [==============================] - 1525s - loss: 0.1038 - acc: 0.9681 - val_loss: 0.0505 - val_acc: 0.9846\n",
    "Epoch 3/10\n",
    "60000/60000 [==============================] - 1454s - loss: 0.0753 - acc: 0.9766 - val_loss: 0.0702 - val_acc: 0.9786\n",
    "Epoch 4/10\n",
    "60000/60000 [==============================] - 1450s - loss: 0.0611 - acc: 0.9817 - val_loss: 0.0355 - val_acc: 0.9884\n",
    "Epoch 5/10\n",
    "60000/60000 [==============================] - 1464s - loss: 0.0527 - acc: 0.9834 - val_loss: 0.0663 - val_acc: 0.9799\n",
    "Epoch 6/10\n",
    "60000/60000 [==============================] - 1418s - loss: 0.0444 - acc: 0.9865 - val_loss: 0.0281 - val_acc: 0.9912\n",
    "Epoch 7/10\n",
    "60000/60000 [==============================] - 1455s - loss: 0.0437 - acc: 0.9872 - val_loss: 0.0409 - val_acc: 0.9868\n",
    "Epoch 8/10\n",
    "60000/60000 [==============================] - 1385s - loss: 0.0387 - acc: 0.9885 - val_loss: 0.0252 - val_acc: 0.9925\n",
    "Epoch 9/10\n",
    "60000/60000 [==============================] - 1476s - loss: 0.0356 - acc: 0.9888 - val_loss: 0.0323 - val_acc: 0.9904\n",
    "Epoch 10/10\n",
    "60000/60000 [==============================] - 1503s - loss: 0.0343 - acc: 0.9896 - val_loss: 0.0273 - val_acc: 0.9917"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
